<!DOCTYPE html>
<html>

<head style="font-family: sans-serif">
  <title>Bowen Wen's Homepage</title>
  <link rel="stylesheet" type="text/css" href="bowen.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-B1GX1SDN9Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-B1GX1SDN9Q');
  </script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>



<body style="max-width: 850px;margin-left: auto;margin-right: auto; margin-bottom: 10px; font-family: sans-serif">
  <table>
    <tbody>
      <tr>
        <td style="padding:0 0 0 0"><img src="images/potrait.jpg" style="width:200px" /></td>
        <td style="vertical-align:top; padding: 0 0 0 50px">
          <h1 style="font-size: 35px">
            Bowen Wen
          </h1>
          I am a Senior Research Scientist at <a href="https://www.nvidia.com/en-us/research/research-areas/">NVIDIA
            Research</a>. My research areas include robotic perception, computer vision. More recently, I focus on large foundation models for 3D visual perception and learning to facilitate robotics or embodied AI. My two leading projects received the best paper award nomination at CVPR 2025 and RSS 2022. I obtained my PhD degree in Computer Science from Rutgers University in 2022, advised
          by Prof. Kostas Bekris. During my PhD, I've spent wonderful summers with <a
            href="https://x.company/">Google[X]</a>, <a
            href="https://www.google.com/url?q=https%3A%2F%2Ftech.fb.com%2Far-vr%2F&sa=D&sntz=1&usg=AFQjCNGsqyZqB8hSHF5DQo8i-dgECsIb2w">Meta
            Reality Labs</a>, <a
            href="https://www.google.com/url?q=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAmazon_Lab126&sa=D&sntz=1&usg=AFQjCNFahov2ghvN4ERAVf_8-LkJomzp8w">Amazon
            Lab 126</a>
          and <a
            href="https://www.google.com/url?q=https%3A%2F%2Fwww.sensetime.com%2Fme-en&sa=D&sntz=1&usg=AFQjCNEB2an64fqkUAL3MXUvX4OUHH5JXQ">SenseTime</a>
          working as a research intern. Prior to my PhD, I obtained MS degree from Ohio State University in 2018 and BS degree from Xi'an Jiaotong University in 2016.

          <p><b>Email:</b> wenbowenxjtu [at] gmail [dot] com</p>

          <p><b>Research Interests:</b> Robotics, Computer Vision, Artificial Intelligence</p>
        </td>
        </td>
    </tbody>
  </table>

  <p style="color:rgb(118, 185, 0); font-size:17px"><b>I'm always looking for self-motivated research interns and open to university collaborations. Please drop me a line if you are interested!</b></p>

  <table style="margin: 50px 0px">
    <tr>
      <td style="padding:0 0px 0px 0;"><a
          href="https://scholar.google.com/citations?user=VSG7Z0kAAAAJ&hl=en&authuser=1"><img src="images/scholar.jpg"
            width=70px, height=70px /></a>
      </td>
      <td style="padding:0 0px 0px 10px;"><a
          href="https://www.google.com/url?q=https%3A%2F%2Fwww.linkedin.com%2Fin%2Fbowen-wen%2F&sa=D&sntz=1&usg=AFQjCNHbDJW-MIEc9Dilkg1V-3jwuuO5cg"><img
            src="images/linkedin.png" width=70px, height=70px /></a>
      </td>
      <td style="padding:0 0px 0px 10px;"><a
          href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fwenbowen123&sa=D&sntz=1&usg=AFQjCNFRNDuGTSEGar53PdBnTSKx0nufDA"><img
            src="images/github.png" width=70px, height=70px /></a>
      </td>
      <td style="padding:0 0px 0px 10px;"><a href="https://www.youtube.com/channel/UCVz7Am0qZOhXDOiY-_waL_g"><img
            src="images/youtube.png" width=70px, height=70px /></a>
      </td>
      <td style="padding:0 0px 0px 10px;"><a href="https://twitter.com/bowenwen_me"><img
        src="images/x_twitter.webp" width=70px, height=70px /></a>
  </td>
    </tr>
  </table>

  <h2>Industry Experience</h2>
  <div>
    <img class="logo_row" src="images/nvidia.png" style="height:30px">
    <img class="logo_row" src="images/x_logo.png" style="height:50px">
    <img class="logo_row" src="images/frl_blue.png" style="height:50px">
    <img class="logo_row" src="images/lab126.png" style="height:40px">
    <img class="logo_row" src="images/sensetime.png" style="height:40px">
  </div>


  <!-- <h2>News</h2> -->
  <ul>
    <!-- <li><span class='gray'>[2024/02]</span> Three papers accepted to CVPR 2024.
    <li><span class='gray'>[2023/06]</span> One paper accepted to CoRL 2023.
    <li><span class='gray'>[2023/06]</span> One paper accepted to IROS 2023.
    <li><span class='gray'>[2023/02]</span> Two papers accepted to CVPR 2023.
    <li><span class='gray'>[2023/01]</span> Two papers accepted to ICRA 2023.
    <li><span class='gray'>[2022/05]</span> I graduated from PhD and will join <a
        href="https://www.nvidia.com/en-us/research/research-areas/">NVIDIA Research</a> as a Research Scientist.
    <li><span class='gray'>[2022/04]</span> One paper accepted to RSS.
    <li><span class='gray'>[2022/01]</span> Three papers accepted to ICRA and one also accepted to RAL.
    <li><span class='gray'>[2021/06]</span> <a href="https://arxiv.org/abs/2108.00516">One paper accepted to IROS
        2021.</a> </li>
    <li><span class='gray'>[2021/05]</span> <a href="https://arxiv.org/abs/2106.14070">One paper accepted to RSS
        2021.</a> </li>
    <li><span class='gray'>[2021/01]</span> I will start my research internship with Google[X] (the moonshot factory),
      in summer 2021.</li> -->
    <!-- <li><span class='gray'>[2020/07]</span> One paper accepted to IROS 2020.</li>
    <li><span class='gray'>[2020/07]</span> One paper accepted to RA-L and will also appear in IROS 2020.</li>
    <li><span class='gray'>[2020/03]</span> Our paper "Tools For Data-Driven Modeling Of Within-Hand Manipulation With
      Underactuated Adaptive Hands" has
      been accepted to L4DC (Learning for Dynamics & Control) 2020.</li>
    <li><span class='gray'>[2020/02]</span> Our paper "Robust, Occlusion-aware Pose Estimation for Objects Grasped by
      Adaptive Hands" has been accepted to
      ICRA 2020.</li>
    <li><span class='gray'>[2020/05]</span> I will start my summer Research Intern at Facebook Reality Labs, Redmond,
      Seattle.</li> -->
    <!-- <li><span class='gray'>[2019/10]</span> Our paper Scene-level Pose Estimation for Multiple Instances of Densely
      Packed Objects has been accepted to
      Conference on Robot Learning (CoRL) - 2019 Spotlight.</li>
    <li><span class='gray'>[2019/09]</span> Our paper Belief-Space Planning using Learned Models with Application to
      Underactuated Hands has been accepted
      to International Symposium on Robotics Research (ISRR) 2019.</li>
    <li><span class='gray'>[2019/05]</span> I will start my summer internship as Applied Scientist in Amazon Lab126, CA
    </li>
    <li><span class='gray'>[2018/09]</span> I started my PhD study in Computer Science in Rutgers University.</li>
    <li><span class='gray'>[2018/05]</span> I will start my Research internship at SenseTime (ÂïÜÊ±§ÁßëÊäÄ), Beijing.</li> -->

  </ul>

  <br>
  <br>
  <h2>Featured Open Source Projects</h2>
  <table style="width: 1000;margin: 0px 0px">
    <tbody>
    <ul>
      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/foundationstereo.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield
          </div>
          <div class="paper_name">
            FoundationStereo: Zero-Shot Stereo Matching
          </div>
          <div class="conference_name">
            CVPR 2025 <b style="color:red">(üèÜ Best Paper Nomination)</b>
          </div>
          <a href="https://arxiv.org/abs/2501.09898">[pdf]</a>
          <a href="https://github.com/NVlabs/FoundationStereo/">[code]</a>
          <a href="https://nvlabs.github.io/FoundationStereo/">[project page]</a>
          <p>
            <a class="github-button" href="https://github.com/NVlabs/FoundationStereo" data-icon="octicon-star"
              data-show-count="true" aria-label="Star NVlabs/FoundationPose on GitHub">Star</a>
            <a class="github-button" href="https://github.com/NVlabs/FoundationStereo/fork" data-icon="octicon-repo-forked"
              data-show-count="true" aria-label="Fork NVlabs/FoundationPose on GitHub">Fork</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/foundationpose_ar_maze.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Wei Yang, Jan Kautz, Stan Birchfield
          </div>
          <div class="paper_name">
            FoundationPose: Unified 6d pose estimation and tracking of novel objects
          </div>
          <div class="conference_name">
            CVPR 2024 <b style="color:red">(Highlight)</b>
          </div>
          <a href="https://arxiv.org/abs/2312.08344">[pdf]</a>
          <a href="https://github.com/NVlabs/FoundationPose">[code]</a>
          <a href="https://nvlabs.github.io/FoundationPose/">[project page]</a>
          <p>
            <a class="github-button" href="https://github.com/NVlabs/FoundationPose" data-icon="octicon-star"
              data-show-count="true" aria-label="Star NVlabs/FoundationPose on GitHub">Star</a>
            <a class="github-button" href="https://github.com/NVlabs/FoundationPose/fork" data-icon="octicon-repo-forked"
              data-show-count="true" aria-label="Fork NVlabs/FoundationPose on GitHub">Fork</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/pumpkin.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz, Stan Birchfield
          </div>
          <div class="paper_name">
            BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects
          </div>
          <div class="conference_name">
            CVPR 2023
          </div>
          <a href="https://arxiv.org/abs/2303.14158">[pdf]</a>
          <a href="https://github.com/NVlabs/BundleSDF">[code]</a>
          <a href="https://bundlesdf.github.io/">[project page]</a>
          <a href="https://www.youtube.com/watch?v=5PymzKbKv8w/">[video]</a>
          <p>
            <a class="github-button" href="https://github.com/NVlabs/BundleSDF" data-icon="octicon-star"
              data-show-count="true" aria-label="Star NVlabs/BundleSDF on GitHub">Star</a>
            <a class="github-button" href="https://github.com/NVlabs/BundleSDF/fork" data-icon="octicon-repo-forked"
              data-show-count="true" aria-label="Fork NVlabs/BundleSDF on GitHub">Fork</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><video autoplay loop muted playsinline src="images/mimicgen.mp4" width=300px, height=150px></video>
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Ajay Mandlekar, Soroush Nasiriany*, <span class="myname">Bowen Wen*</span>, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, Dieter Fox <b style="color:black">(* equal contribution)</b>
          </div>
          <div class="paper_name">
            MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations
          </div>
          <div class="conference_name">
            CoRL 2023
          </div>
          <a href="https://arxiv.org/abs/2310.17596">[pdf]</a>
          <a href="https://github.com/NVlabs/mimicgen_environments">[code]</a>
          <a href="https://mimicgen.github.io/">[project page]</a>
          <p>
            <a class="github-button" href="https://github.com/NVlabs/mimicgen_environments" data-icon="octicon-star"
              data-show-count="true" aria-label="Star NVlabs/mimicgen_environments on GitHub">Star</a>
            <a class="github-button" href="https://github.com/NVlabs/mimicgen_environments/fork" data-icon="octicon-repo-forked"
              data-show-count="true" aria-label="Fork NVlabs/mimicgen_environments on GitHub">Fork</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/catgrasp.jpg" width=300px, height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Wenzhao Lian, Kostas Bekris, Stefan Schaal
          </div>
          <div class="paper_name">
            CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation
          </div>
          <div class="conference_name">
            ICRA 2022
          </div>
          <a href="https://arxiv.org/abs/2109.09163">[pdf]</a>
          <a href="https://sites.google.com/view/catgrasp">[code]</a>
          <a href="https://www.youtube.com/watch?v=rAX-rFSKAto">[video]</a>
          <p>
            <a class="github-button" href="https://github.com/wenbowen123/catgrasp" data-icon="octicon-star"
              data-show-count="true" aria-label="Star wenbowen123/catgrasp on GitHub">Star</a>
            <a class="github-button" href="https://github.com/wenbowen123/catgrasp/fork" data-icon="octicon-repo-forked"
              data-show-count="true" aria-label="Fork wenbowen123/catgrasp on GitHub">Fork</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/vis_scene_1_method_ours_c.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Kostas E. Bekris
          </div>
          <div class="paper_name">
          BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models
          </div>
          <div class="conference_name">
            IROS 2021
          </div>
          <a href="https://arxiv.org/abs/2108.00516">[pdf]</a>
          <a href="https://github.com/wenbowen123/BundleTrack">[code]</a>
          <a href="https://www.youtube.com/watch?v=0UorLR0ADd4">[presentation video]</a>
          <a href="https://www.youtube.com/watch?v=89pnv3M_84g">[supplementary video]</a>
          <p>
            <a class="github-button" href="https://github.com/wenbowen123/BundleTrack" data-icon="octicon-star"
              data-show-count="true" aria-label="Star wenbowen123/BundleTrack on GitHub">Star</a>
            <a class="github-button" href="https://github.com/wenbowen123/BundleTrack/fork"
              data-icon="octicon-repo-forked" data-show-count="true"
              aria-label="Fork wenbowen123/BundleTrack on GitHub">Fork</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/se3_tracknet.gif" width=300px, height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Chaitanya Mitash, Baozhang Ren, Kostas Bekris
          </div>
          <div class="paper_name">
            se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains
          </div>
          <div class="conference_name">
            IROS 2020
          </div>
          <a href="https://arxiv.org/pdf/2007.13866.pdf">[pdf]</a>
          <a href="https://github.com/wenbowen123/iros20-6d-pose-tracking">[code]</a>
          <a href="https://www.youtube.com/watch?v=dhqM0hZmGR4">[supplementary video]</a>
          <a href="https://www.youtube.com/watch?v=5pH3PnZSgoY&t=3s">[presentation video]</a>
          <p>
            <a class="github-button" href="https://github.com/wenbowen123/iros20-6d-pose-tracking"
              data-icon="octicon-star" data-show-count="true"
              aria-label="Star wenbowen123/iros20-6d-pose-tracking on GitHub">Star</a>
            <a class="github-button" href="https://github.com/wenbowen123/iros20-6d-pose-tracking/fork"
              data-icon="octicon-repo-forked" data-show-count="true"
              aria-label="Fork wenbowen123/iros20-6d-pose-tracking on GitHub">Fork</a>
          </p>
        </td>
      </tr>

    </ul>
    </tbody>
  </table>

  <h2>Recent Projects</h2>
  <table style="width: 1000;margin: 0px 0px">
    <tbody>
      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/any6d.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Taeyeop Lee, <span class="myname">Bowen Wen</span>, Minjun Kang, Gyuree Kang, In So Kweon, Kuk-Jin Yoon
          </div>
          <div class="paper_name">
            Any6D: Model-free 6D Pose Estimation of Novel Objects
          </div>
          <div class="conference_name">
            CVPR 2025
          </div>
          <a href="https://arxiv.org/pdf/2503.18673">[pdf]</a>
          <a href="https://github.com/taeyeopl/Any6D">[code]</a>
          <a href="http://taeyeop.com/any6d">[project page]</a>
          <p>
            <a class="github-button" href="https://github.com/taeyeopl/Any6D" data-icon="octicon-star"
              data-show-count="true" aria-label="Star NVlabs/FoundationPose on GitHub">Star</a>
            <a class="github-button" href="https://github.com/taeyeopl/Any6D/fork" data-icon="octicon-repo-forked"
              data-show-count="true" aria-label="Fork NVlabs/FoundationPose on GitHub">Fork</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/spot.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Cheng-Chun Hsu, <span class="myname">Bowen Wen <span class="email_icon"></span></span>, Jie Xu, Yashraj Narang, Xiaolong Wang, Yuke Zhu, Joydeep Biswas, Stan Birchfield <b style="color:black">(<span class="email_icon"></span> project leader)</b>
          </div>
          <div class="paper_name">
            SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation
          </div>
          <div class="conference_name">
            ICRA 2025</b>
          </div>
          <a href="https://arxiv.org/pdf/2411.00965">[pdf]</a>
          <a href="https://github.com/NVlabs/object_centric_diffusion">[code]</a>
          <a href="https://nvlabs.github.io/object_centric_diffusion/">[project page]</a>
          <p>
            <a class="github-button" href="https://github.com/NVlabs/object_centric_diffusion" data-icon="octicon-star"
              data-show-count="true" aria-label="Star NVlabs/FoundationPose on GitHub">Star</a>
            <a class="github-button" href="https://github.com/NVlabs/object_centric_diffusion/fork" data-icon="octicon-repo-forked"
              data-show-count="true" aria-label="Fork NVlabs/FoundationPose on GitHub">Fork</a>
          </p>
        </td>
      </tr>




      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/artnetf.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Yijia Weng, <span class="myname">Bowen Wen <span class="email_icon"></span></span>, Jonathan Tremblay, Valts Blukis, Dieter Fox, Leonidas Guibas, Stan Birchfield <b style="color:black">(<span class="email_icon"></span> project leader)</b>
          </div>
          <div class="paper_name">
            Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects
          </div>
          <div class="conference_name">
            CVPR 2024
          </div>
          <a href="https://arxiv.org/abs/2404.01440">[pdf]</a>
          <a href="https://github.com/NVlabs/DigitalTwinArt">[code]</a>
          <a href="https://nvlabs.github.io/DigitalTwinArt/">[project page]</a>
          <p>
            <a class="github-button" href="https://github.com/NVlabs/DigitalTwinArt" data-icon="octicon-star"
              data-show-count="true" aria-label="Star NVlabs/DigitalTwinArt on GitHub">Star</a>
            <a class="github-button" href="https://github.com/NVlabs/DigitalTwinArt/fork" data-icon="octicon-repo-forked"
              data-show-count="true" aria-label="Fork NVlabs/DigitalTwinArt on GitHub">Fork</a>
          </p>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/rss_automate.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Bingjie Tang, Iretiayo Akinola, Jie Xu, <span class="myname">Bowen Wen</span>, Ankur Handa, Karl Van Wyk, Dieter Fox, Gaurav S Sukhatme, Fabio Ramos, Yashraj Narang
          </div>
          <div class="paper_name">
            AutoMate: Specialist and Generalist Assembly Policies over Diverse Geometries
          </div>
          <div class="conference_name">
            RSS 2024
          </div>
          <a href="https://arxiv.org/abs/2407.08028">[pdf]</a>
          <a href="https://github.com/isaac-sim/IsaacGymEnvs/blob/automate/docs/automate.md">[code]</a>
          <a href="https://bingjietang718.github.io/automate/">[project page]</a>
        </td>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><video autoplay loop muted playsinline src="images/skill_mimicgen.mp4" width=300px, height=150px></video>
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Caelan Garrett*, Ajay Mandlekar*, <span class="myname">Bowen Wen</span>, Dieter Fox
          </div>
          <div class="paper_name">
            SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment
          </div>
          <div class="conference_name">
            CoRL 2024
          </div>
          <a href="https://arxiv.org/pdf/2410.18907">[pdf]</a>
          <a href="https://skillgen.github.io/">[project page]</a>
        </td>
      </tr>






      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/battery_assembly_c.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Wenzhao Lian, Kostas Bekris, Stefan Schaal
          </div>
          <div class="paper_name">
            You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration
          </div>
          <div class="conference_name">
            RSS 2022 <b style="color:red">(üèÜ Best Paper Nomination)</b>
          </div>
          <a href="https://arxiv.org/abs/2201.12716">[pdf]</a>
          <a href="https://github.com/wenbowen123/BundleTrack">[code of tracking]</a>
          <a href="https://github.com/wenbowen123/catgrasp">[code of grasping]</a>
          <a href="https://www.youtube.com/watch?v=WAr8ZY3mYyw">[video]</a>
        </td>
      </tr>


      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/finger_gaiting_bunny.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Andrew Morgan*, Kaiyu Hang*, <span class="myname">Bowen Wen</span>, Kostas E Bekris, Aaron Dollar
          </div>
          <div class="paper_name">
            Complex In-Hand Manipulation via Compliance-Enabled Finger Gaiting and Multi-Modal Planning
          </div>
          <div class="conference_name">
            IEEE Robotics and Automation Letters 2022
          </div>
          <a href="https://arxiv.org/pdf/2201.07928.pdf">[pdf]</a>
          <a href="https://www.youtube.com/watch?v=6QkRWfFt3RI&t=4s">[video]</a>
          <a
            href="https://yaledailynews.com/blog/2022/03/17/freed-to-fidget-robotic-hands-manipulate-everyday-objects/">[news
            coverage]</a>
      </tr>
      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/stacking.gif" width=300px, height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Junchi Liang, <span class="myname">Bowen Wen</span>, Kostas Bekris, Abdeslam Boularias
          </div>
          <div class="paper_name">
            Learning Sensorimotor Primitives of Sequential Manipulation Tasks from Visual Demonstrations
          </div>
          <div class="conference_name">
            ICRA 2022
          </div>
          <a href="https://arxiv.org/pdf/2203.03797.pdf">[pdf]</a>
          <a href="https://www.youtube.com/watch?v=_UIXKaf_2WU">[video]</a>
      </tr>

      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/cup_stacking_and_charger.gif" width=300px,
            height=150px />
        </td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Andrew S. Morgan*, <span class="myname">Bowen Wen*</span>, Junchi Liang, Abdeslam Boularias, Aaron M. Dollar, Kostas Bekris <b style="color:black">(* equal contribution)</b>
          </div>
          <div class="paper_name">
            Vision-driven Compliant Manipulation for Reliable, High-Precision Assembly Tasks
          </div>
          <div class="conference_name">
            RSS 2021
          </div>
          <br>
          <a href="https://arxiv.org/abs/2106.14070">[pdf]</a>
          <a href="https://www.youtube.com/watch?v=3blr7u3VA7Y&t=40s">[video]</a>
        </td>
      </tr>

      <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/ral2020.gif" width=300px, height=150px /></td>
      <td style="vertical-align:top;font-size: 15px">
        <div class="author_name">
          Mitash, Chaitanya, Shome, Rahul, <span class="myname">Wen, Bowen</span>, Boularias, Abdeslam, Bekris, Kostas
        </div>
        <div class="paper_name">
          Task-driven Perception and Manipulation for Constrained Placement with No Shape Priors
        </div>
        <div class="conference_name">
          RA-L and IROS 2020
        </div>
        <a href="https://arxiv.org/pdf/2006.15503.pdf">[pdf]</a>
        <a href="https://www.youtube.com/watch?v=uVkn7y3S0sw">[video]</a>
      </td>
      </td>
      <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/icra2020.jpg" width=300px, height=150px /></td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Chaitanya Mitash, Sruthi Soorian, Andrew Kimmel, Avishai Sintov, Kostas E. Bekris
          </div>
          <div class="paper_name">
            Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands
          </div>
          <div class="conference_name">
            IEEE International Conference on Robotics and Automation (ICRA) 2020
          </div>
          <a href="https://arxiv.org/pdf/2003.03518.pdf">[pdf]</a>
          <a href="https://github.com/wenbowen123/icra20-hand-object-pose">[code]</a>
          <a href="https://www.youtube.com/watch?v=jCt0-dJAvgI&t=1s">[video]</a>
          <p>
            <a class="github-button" href="https://github.com/wenbowen123/icra20-hand-object-pose"
              data-icon="octicon-star" data-show-count="true"
              aria-label="Star wenbowen123/icra20-hand-object-pose on GitHub">Star</a>
            <a class="github-button" href="https://github.com/wenbowen123/icra20-hand-object-pose/fork"
              data-icon="octicon-repo-forked" data-show-count="true"
              aria-label="Fork wenbowen123/icra20-hand-object-pose on GitHub">Fork</a>
          </p>
        </td>
      </tr>
      <!-- <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/l4dc2019.jpg" width=300px, height=150px /></td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Avishai Sintov, Andrew Kimmel, <span class="myname">Bowen Wen</span>, Abdeslam Boularias, Kostas Bekris
          </div>
          <div class="paper_name">
            "Tools for Data-driven Modeling of Within-Hand Manipulation with Underactuated Adaptive Hands"
          </div>
          <div class="conference_name">
            L4DC (Learning for Dynamics & Control) 2020
          </div>
          <a href="http://proceedings.mlr.press/v120/sintov20a/sintov20a.pdf">[pdf]</a>
          <a href="https://github.com/avishais/underactuated_hand_tools">[code]</a>
        </td>
      </tr> -->
      <!-- <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/isrr2019.jpg" width=300px, height=150px /></td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Andrew Kimmel, Avishai Sintov, Juntao Tan, <span class="myname">Bowen Wen</span>, Abdeslam Boularias, Kostas Bekris
          </div>
          <div class="paper_name">
            Belief-Space Planning Using Learned Models With Application To Underactuated Hands
          </div>
          <div class="conference_name">
            International Symposium on Robotics Research (ISRR) 2019
          </div>
          <a href="http://rl.cs.rutgers.edu/publications/ISRR2019.pdf">[pdf]</a>
          <a href="https://www.youtube.com/watch?v=54G5BvEeE8E">[video]</a>
        </td>
      </tr> -->

      <!-- <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/corl2019.jpg" width=300px, height=150px /></td>
        <td style="vertical-align:top;font-size: 15px">
          <div class="author_name">
            Chaitanya Mitash, <span class="myname">Bowen Wen</span>, Kostas Bekris, Abdeslam Boularias
          </div>
          <div class="paper_name">
            Scene-level Pose Estimation for Multiple Instances of Densely Packed Objects
          </div>
          <div class="conference_name">
          Conference on Robot Learning (CoRL) 2019
          </div>
          <a href="https://arxiv.org/pdf/1910.04953.pdf">[pdf]</a>
          <a href="https://github.com/cmitash/multi-instance-pose-estimation">[code]</a>
          <a href="https://www.youtube.com/watch?v=BN4Q5Gs9xQg">[video]</a>
        </td>
      </tr> -->

      <!-- <tr>
        <td style="width: 200px;padding:0 15px 40px 0;"><img src="images/adl2018.jpg" width=300px, height=150px /></td>
        <td style="vertical-align: top; font-size: 15px; padding-bottom: 10px;">
          <div class="author_name">
            <span class="myname">Bowen Wen</span>, Sukru Yaren Gelbal, Bilin Aksun Guvenc, Levent Guvenc
          </div>
          <div class="paper_name">
            Localization and Perception for Control and Decision-Making of a Low-Speed Autonomous Shuttle in a Campus Pilot Deployment.
          </div>
          <div class="conference_name">
            SAE International Journal of Connected and Automated Vehicles 2018
          </div>
          <a href="https://par.nsf.gov/servlets/purl/10076473">[pdf]</a>
          <a href="https://github.com/wenbowen123/hector_slam_Ceres">[code]</a>
          <a href="https://www.youtube.com/watch?v=LwrdPRxHzrg">[video]</a>
          <p>
            <a class="github-button" href="https://github.com/wenbowen123/hector_slam_Ceres" data-icon="octicon-star"
              data-show-count="true" aria-label="Star wenbowen123/hector_slam_Ceres on GitHub">Star</a>
            <a class="github-button" href="https://github.com/wenbowen123/hector_slam_Ceres/fork"
              data-icon="octicon-repo-forked" data-show-count="true"
              aria-label="Fork wenbowen123/hector_slam_Ceres on GitHub">Fork</a>
          </p>
        </td>
      </tr> -->
    </tbody>
  </table>


  <h2>Academic Services</h2>

  <ul>
    <li>Invited Chair for IROS (IEEE/RSJ International Conference on Intelligent Robots and Systems) session "Visual
      Tracking II" 2021</li>

    <li>Reviewer for following conferences and journals:
      <ul>
        <li>ICCV</li>
        <li>ECCV</li>
        <li>ICML</li>
        <li>NeurIPS</li>
        <li>TPAMI (IEEE Transactions on Pattern Analysis and Machine Intelligence)</li>
        <li>TRO (IEEE Transactions on Robotics)</li>
        <li>ISPRS Journal of Photogrammetry and Remote Sensing</li>
        <li>RAL (IEEE Robotics and Automation Letters)</li>
        <li>ICRA (International Conference on Robotics and Automation)</li>
        <li>IROS (IEEE/RSJ International Conference on Intelligent Robots and Systems)</li>
        <li>CoRL (Conference on Robot Learning)</li>
        <li>Neurocomputing</li>
        <!-- <li>ICVS (International Conference on Computer Vision Systems)</li>
        <li>IECON (Annual Conference of the IEEE Industrial Electronics Society (IES))</li> -->
      </ul>
    </li>

  </ul>


</body>

<br>
<br>
<br>
<br>

<p style="text-align: center;font-size: 12px">&copyBowen Wen. All rights reserved</p>

<script>
  function createEmailIconElement() {
    const img = document.createElement('img');
    img.src = 'images/e-mail.svg';
    img.style.height = '1em';
    img.style.verticalAlign = 'middle';
    return img;
  }
  document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.email_icon').forEach(placeholder => {
      placeholder.innerHTML = ''; // Clear existing content if any
      placeholder.appendChild(createEmailIconElement());
    });
  });
</script>

</html>